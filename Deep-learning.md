- **感知机**
- **神经网络**
	- **引入**
		从感知机到神经网络
		![](https://pic4.zhimg.com/80/v2-8037991df33290b222bf43a05b78e37f_1440w.webp)
		感知机接收x 1 和x 2 两个输入信号，输出y。如果用数学式来表示感知机，则如式（3.1）所示
		![](https://pic2.zhimg.com/80/v2-7da9e1f699d1ec13f3c7df5641b40ed1_1440w.webp)
		引入新函数h(x)（激活函数登场），将式（3.1）简化改写成下面的式（3.2）和式（3.3）。
		y = h(b + w 1 x 1 + w 2 x 2 ) （3.2）
		![](https://pic1.zhimg.com/80/v2-feb070b4b8f3f6d578e4d9a3618c6ac4_1440w.webp)
		激活函数登场！
		激活函数是连接感知机和神经网络的桥梁。
		登场的h（x）函数会将输入信号的总和转换为输出信号，这种函数一般称为激活函数（activation function）。如“激活”一词所示，激活函数的作用在于决定如何来激活输入信号的总和。
		式（3.3）表示的激活函数以阈值为界，一旦输入超过阈值，就切换输出。这样的函数称为“阶跃函数”。
		可以说感知机中使用了阶跃函数作为激活函数。也就是说，在激活函数的众多候选函数中，感知机使用了阶跃函数。那么，如果感知机使用其他函数作为激活函数的话会怎么样呢？实际上，如果将激活函数从阶跃函数换成其他函数，就可以进入神经网络的世界了。
		神经网络使用的激活函数
		神经网络中经常使用的一个激活函数就是式（3.6）表示的sigmoid函数（sigmoid function）。
		![](https://pic3.zhimg.com/80/v2-9f40b0243267d48810f53955d5a47f66_1440w.webp)
		神经网络中用sigmoid函数作为激活函数，进行信号的转换，转换后的信号被传送给下一个神经元。
		sigmoid 函数和阶跃函数的比较
		![](https://pic2.zhimg.com/80/v2-2a729bfc933a7318e617c231f8d91259_1440w.webp)
		不同性质：
		首先是“平滑性”的不同。sigmoid函数是一条平滑的曲线，输出随着输入发生连续性的变化。而阶跃函数以0为界，输出发生急剧性的变化。**sigmoid函数的平滑性对神经网络的学习具有重要意义。**
		另一个不同是相对于阶跃函数只能返回0或1，sigmoid函数可以返回0.731 . . . 、0.880 . . . 等实数（这一点和刚才的平滑性有关）。也就是说，感知机中神经元之间流动的是0或1的二元信号，而神经网络中流动的是连续的实数值信号。
		共同性质：
		首先是当输入信号为重要信息时，阶跃函数和sigmoid函数都会输出较大的值；当输入信号为不重要的信息时，两者都输出较小的值。
		另一个是不管输入信号有多小，或者有多大，输出信号的值都在0到1之间。
		最后，两者均为**非线性函数**
		为什么要非线性函数？
		激活函数不能使用线性函数。因为使用线性函数的话，加深神经网络的层数就没有意义了。线性函数的问题在于，不管如何加深层数，总是存在与之等效的“无隐藏层的神经网络”。因此，为了发挥叠加层所带来的优势，激活函数必须使用非线性函数。
		ReLU函数
		ReLU函数在输入大于0时，直接输出该值；在输入小于等于0时，输出0
		![](https://pic4.zhimg.com/80/v2-ebd1d5b6c818ce0174b3ecc36b49ecb7_1440w.webp)
		sigmoid函数很早就开始被使用了，而最近则主要使用ReLU（Rectified Linear Unit）函数
	- **前向处理**
	- **反向传播**
	- **学习**
		- **参数的更新**
		- **权值和BN**
		- **正则化**
		- **超参数**
	- **预处理和批处理**
- **卷积神经网络(CNN)**
	CNN用于图像识别，在图像识别的比赛中，基于深度学习的方法几乎都以CNN为基础。
	- **整体结构**
		CNN和之前介绍的神经网络一样，可以像乐高积木一样通过组装层来构建。
		不过，CNN中新出现了卷积层（Convolution层）和池化层（Pooling层）。
		之前介绍的神经网络中，相邻层的所有神经元之间都有连接，这称为全连接（fully-connected）。我们用Affine层实现了全连接层。
		
		![](https://pic2.zhimg.com/80/v2-4a39981a5cc80380f11d90904615bbe1_1440w.webp)
		
		全连接的神经网络中，Affine层后面跟着激活函数ReLU层（或者Sigmoid层）
		这里堆叠了4层“Affine-ReLU”组合，然后第5层是Affine层，最后由Softmax层输出最终结果（概率）
		
		![](https://pic3.zhimg.com/80/v2-1e9e2e8fff99e36f1a8df498b17a7b36_1440w.webp)
		
		CNN中新增了Convolution层和Pooling层
		CNN的层的连接顺序是“Convolution - ReLU -（Pooling）”
		
		在图7-2的CNN中，靠近输出的层中使用了之前的“Affine - ReLU”组合。此外最后的输出层中使用了之前的“Affine -Softmax”组合。这些都是一般的CNN中比较常见的结构
		
		全连接层存在什么问题呢？那就是数据的形状被“忽视”了。比如，输入数据是图像时，图像通常是高、长、通道方向上的3维形状。但是，向全连接层输入时，需要将3维数据拉平为1维数据。全连接层会忽视形状，将全部的输入数据作为相同的神经元（同一维度的神经元）处理，所以无法利用与形状相关的信息。
		
		而卷积层可以保持形状不变。当输入数据是图像时，卷积层会以3维数据的形式接收输入数据，并同样以3维数据的形式输出至下一层。因此在CNN中，可以（有可能）正确理解图像等具有形状的数据。
	- **卷积层**
		CNN中，有时将卷积层的输入输出数据称为特征图（feature map）。
		其中，卷积层的输入数据称为输入特征图（input feature map），输出数据称为输出特征图（output feature map）。
		卷积层进行的处理就是卷积运算。卷积运算相当于图像处理中的“滤波器运算”。
		![](https://pic4.zhimg.com/80/v2-2d17f0391ab1819044a4e0381f05e0d3_1440w.webp)
		在本例中，输入大小是(4, 4)，滤波器大小是(3, 3)，输出大小是(2, 2)。
		
		对于输入数据，卷积运算以一定间隔滑动滤波器的窗口并应用。如图7-4所示，将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（有时将这个计算称为乘积累加运算）。然后，将这个结果保存到输出的对应位置。将这个过程在所有位置都进行一遍，就可以得到卷积运算的输出。
		![](https://pic4.zhimg.com/80/v2-f2eea7512899fddc7ddf24c7fc4a886f_1440w.webp)
		
		在全连接的神经网络中，除了权重参数，还存在偏置。CNN中，滤波器的参数就对应之前的权重。并且，CNN中也存在偏置。包含偏置的卷积运算的处理流如图7-5所示。
		
		![](https://pic3.zhimg.com/80/v2-6a2ef9dec685d5021dcf393ff67db40a_1440w.webp)
		
		本例中，相对于应用了滤波器的4个数据，偏置只有1个
		
		**填充**（padding）
		
		在进行卷积层的处理之前，有时要向输入数据的周围填入固定的数据（比如0等），这称为填充，是卷积运算中经常会用到的处理。如图7-6所示
		
		![](https://pic3.zhimg.com/80/v2-0e2311a49a8a738275eb13959ae2dc06_1440w.webp)
		
		通过填充，大小为(4, 4)的输入数据变成了(6, 6)的形状。然后，应用大小为(3, 3)的滤波器，生成了大小为(4, 4)的输出数据。
		
		使用填充主要是为了调整输出的大小。比如，对大小为(4, 4)的输入数据应用(3, 3)的滤波器时，输出大小变为(2, 2)，相当于输出大小比输入大小缩小了2个元素。如果每次进行卷积运算都会缩小空间，那么在某个时刻输出大小就有可能变为1，导致无法再应用卷积运算。为了避免出现这样的情况，就要使用填充。
		
		**步幅**（stride）
		
		应用滤波器的位置间隔称为步幅（stride）。之前的例子中步幅都是1，如果将步幅设为2，则如图7-7所示，应用滤波器的窗口的间隔变为2个元素。
		
		![](https://pic1.zhimg.com/80/v2-2f76fb94d996492e979c0dbfe665e054_1440w.webp)
		
		对输入大小为(7, 7)的数据，以步幅2应用了滤波器。通过将步幅设为2，输出大小变为(3, 3)。
		
		综上，增大步幅后，输出大小会变小。而增大填充后，输出大小会变大。
		
		假设输入大小为(H, W)，滤波器大小为(FH, FW)，输出大小为(OH, OW)，填充为P，步幅为S。此时，输出大小可通过式(7.1)进行计算。
		
		![](https://pic2.zhimg.com/80/v2-dfdee6594e16194052e5ef20ae8b2305_1440w.webp)
		
		**3维数据**的卷积运算
		
		之前的卷积运算的例子都是以有高、长方向的2维形状为对象的。但是，图像是3维数据，除了高、长方向之外，还需要处理通道方向。
		
		图7-8是卷积运算的例子，图7-9是计算顺序。这里以3通道的数据为例，展示了卷积运算的结果。
		
		![](https://pic3.zhimg.com/80/v2-4776d8df93015fd04a7bfbf52f839042_1440w.webp)
		
		需要注意的是，在3维数据的卷积运算中，输入数据和滤波器的通道数要设为相同的值。
		
		![](https://pic2.zhimg.com/80/v2-94a7b140abc2721bd56179cc3f9c0db9_1440w.webp)
		
		通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出
		
		将数据和滤波器结合长方体的方块来考虑，3维数据的卷积运算会很容易理解。方块是如图7-10所示的3维长方体。把3维数据表示为多维数组时，书写顺序为（channel, height, width）。
		
		![](https://pic4.zhimg.com/80/v2-529ec5df79e6682f64706a74df957893_1440w.webp)
		
		通道数为C、高度为H、长度为W的数据的形状可以写成（C, H, W）。
		
		在这个例子中，数据输出是1张特征图。所谓1张特征图，换句话说，就是通道数为1的特征图。那么，如果要在通道方向上也拥有多个卷积运算的输出，该怎么做呢？为此，就需要用到多个滤波器（权重）。用图表示的话，如图7-11所示。
		
		![](https://pic1.zhimg.com/80/v2-fe8cc3818d922c469d1f1fdeceb633f8_1440w.webp)
		
		通过应用FN个滤波器，输出特征图也生成了FN个。如果将这FN个特征图汇集在一起，就得到了形状为(FN, OH, OW)的方块。将这个方块传给下一层，就是CNN的处理流。
		
		作为4维数据，滤波器的权重数据要按(output_channel, input_channel, height, width)的顺序书写。比如，通道数为3、大小为5 × 5的滤波器有20个时，可以写成(20, 3, 5, 5)。
		
		卷积运算中（和全连接层一样）存在偏置。在图7-11的例子中，如果进一步追加偏置的加法运算处理，则结果如下面的图7-12所示。
		
		![](https://pic2.zhimg.com/80/v2-ef1f9fd7a99f20e5ba0773cc1cbe9f25_1440w.webp)
		
		每个通道只有一个偏置。这里，偏置的形状是(FN, 1, 1)，滤波器的输出结果的形状是(FN, OH, OW)。这两个方块相加时，要对滤波器的输出结果(FN, OH, OW)按通道加上相同的偏置值。
		
		**批处理**
		
		神经网络的处理中进行了将输入数据打包的批处理。通过批处理，能够实现处理的高效化和学习时对mini-batch的对应。
		
		我们希望卷积运算也同样对应批处理。为此，需要将在各层间传递的数据保存为4维数据。具体地讲，就是按(batch_num, channel, height, width)的顺序保存数据。比如，将图7-12中的处理改成对N个数据进行批处理时，数据的形状如图7-13所示。
		
		![](https://pic3.zhimg.com/80/v2-f378f04c1e0ac4cdd22ab04ffe6d1746_1440w.webp)
		
		数据作为4维的形状在各层间传递。这里需要注意的是，网络间传递的是4维数据，对这N个数据进行了卷积运算。也就是说，批处理将N次的处理汇总成了1次进行。
	- **池化层**
		池化是缩小高、长方向上的空间的运算。
		如图7-14所示，进行将2 × 2的区域集约成1个元素的处理，缩小空间大小。
		![](https://pic4.zhimg.com/80/v2-63b48cab99015ce64e8a2e3aa153c207_1440w.webp)
		
		图中是按步幅2进行2 × 2的Max池化时的处理顺序。“Max池化”是获取最大值的运算，“2 × 2”表示目标区域的大小。
		
		一般来说，池化的窗口大小会和步幅设定成相同的值。比如，3 × 3的窗口的步幅会设为3，4 × 4的窗口的步幅会设为4等。
		
		除了Max池化之外，还有Average池化等。相对于Max池化是从目标区域中取出最大值，Average池化则是计算目标区域的平均值。在图像识别领域，主要使用Max池化。
		
		池化层有以下特征：
		**没有要学习的参数**
		池化层和卷积层不同，没有要学习的参数。池化只是从目标区域中取最大值（或者平均值），所以不存在要学习的参数。
		**通道数不发生变化**
		经过池化运算，输入数据和输出数据的通道数不会发生变化。如图7-15所示，计算是按通道独立进行的。
		**对微小的位置变化具有鲁棒性**（健壮）
		输入数据发生微小偏差时，池化仍会返回相同的结果。
		![](https://pic1.zhimg.com/80/v2-38c37ed5c6fb553e0260500a45fd7268_1440w.webp)
		
		计算是按通道独立进行的
		![](https://pic3.zhimg.com/80/v2-24008f6133c12220ba250d553a6211ba_1440w.webp)
		
		池化会吸收输入数据的偏差（根据数据的不同，结果有可能不一致）。
	- **实现**
		CNN中处理的是4维数据，因此卷积运算的实现看上去会很复杂，但是通过使用下面要介绍的 im2col （image to column）这个技巧，问题就会变得很简单。
		
		im2col 是一个函数，将输入数据展开以适合滤波器（权重）。如图7-17所示，对3维的输入数据应用 im2col 后，数据转换为2维矩阵。
		
		![](https://pic4.zhimg.com/80/v2-fac6cc04bf07ca43e1c82620684af82b_1440w.webp)
		
		如图7-18所示，对于输入数据，将应用滤波器的区域（3维方块）横向展开为1列。 im2col 会
		
		在所有应用滤波器的地方进行这个展开处理。
		
		![](https://pic4.zhimg.com/80/v2-e8299d3c18783f378d950020c275a69f_1440w.webp)
		
		在图7-18中，为了便于观察，将步幅设置得很大，以使滤波器的应用区域不重叠。而在实际的卷积运算中，滤波器的应用区域几乎都是重叠的。在滤波器的应用区域重叠的情况下，使用 im2col 展开后，展开后的元素个数会多于原方块的元素个数。因此，使用 im2col 的实现存在比普通的实现消耗更多内存的缺点。
		
		使用 im2col 展开输入数据后，之后就只需将卷积层的滤波器（权重）纵向展开为1列，并计算2个矩阵的乘积即可（参照图7-19）。
		
		如图7-19所示，基于 im2col 方式的输出结果是2维矩阵。因为CNN中数据会保存为4维数组，所以要将2维输出数据转换为合适的形状。以上就是卷积层的实现流程。
		
		![](https://pic1.zhimg.com/80/v2-bed246116f4d969fa55932b2bfcbcf5c_1440w.webp)
		
		池化层的实现和卷积层相同，都使用 im2col 展开输入数据。不过，池化的情况下，在通道方向上是独立的，这一点和卷积层不同。具体地讲，如图7-21所示，池化的应用区域按通道单独展开。
		
		![](https://pic1.zhimg.com/80/v2-a1c5426a8015d9d1941c8e913ff268b0_1440w.webp)
		
		展开之后，只需对展开的矩阵求各行的最大值，并转换为合适的形状即可（图7-22）。
		
		![](https://pic1.zhimg.com/80/v2-5ef671f0de401465f7dfa2972a82e00c_1440w.webp)
		
		如图7-22所示，池化层的实现按下面3个阶段进行。
		
		1.展开输入数据。
		
		2.求各行的最大值。
		
		3.转换为合适的输出大小
	- **可视化**
		CNN中用到的卷积层在“观察”什么呢？
		第1层权重的可视化
		我们对MNIST数据集进行了简单的CNN学习。第1层的卷积层的权重的形状是(30, 1, 5, 5)，即30个大小为5 × 5、通道为1的滤波器。滤波器大小是5 × 5、通道数是1，意味着滤波器可以可视化为1通道的灰度图像。现在，我们将卷积层（第1层）的滤波器显示为图像。这里，我们来比较一下学习前和学习后的权重，结果如图7-24所示
		
		![](https://pic4.zhimg.com/80/v2-863c25c8ed6054018444a32aaf20cd2b_1440w.webp)
		
		通过学习，滤波器被更新成了有规律的滤波器，比如从白到黑渐变的滤波器、含有块状区域（称为blob）的滤波器等
		
		图7-24中，学习前的滤波器是随机进行初始化的，所以在黑白的浓淡上没有规律可循，但学习后的滤波器变成了有规律的图像。
		
		图7-24中右边的有规律的滤波器在“观察”什么？它在观察边缘（颜色变化的分界线）和斑块（局部的块状区域）等。比如，左半部分为白色、右半部分为黑色的滤波器的情况下，如图7-25所示，会对垂直方向上的边缘有响应。
		
		![](https://pic2.zhimg.com/80/v2-ae95747248030d7021c2f2ee20e4af91_1440w.webp)
		
		“滤波器1”对垂直方向上的边缘有响应，“滤波器2”对水平方向上的边缘有响应。
		
		由此可见，卷积层的滤波器会提取边缘或斑块等原始信息。而刚才实现的CNN会将这些原始信息传递给后面的层。
		
		基于分层结构的信息提取
		
		第1层的卷积层中提取了边缘或斑块等“低级”信息，那么在堆叠了多层的CNN中，各层中又会提取什么样的信息呢？根据深度学习的可视化相关的研究，随着层次加深，提取的信息（正确地讲，是反映强烈的神经元）也越来越抽象。
		
		图7-26中展示了进行一般物体识别（车或狗等）的8层CNN（AlexNet）。AlexNet网络结构堆叠了多层卷积层和池化层，最后经过全连接层输出结果。图7-26的方块表示的是中间数据，对于这些中间数据，会连续应用卷积运算。
		
		![](https://pic2.zhimg.com/80/v2-db3840960045c809b666fae0f7c7f3b9_1440w.webp)
		
		如果堆叠了多层卷积层，则随着层次加深，提取的信息也愈加复杂、抽象，这是深度学习中很有意思的一个地方。最开始的层对简单的边缘有响应，接下来的层对纹理有响应，再后面的层对更加复杂的物体部件有响应。也就是说，随着层次加深，神经元从简单的形状向“高级”信息变化。
		
		小结
		
		• CNN在此前的全连接层的网络中新增了卷积层和池化层。
		
		• 使用 im2col 函数可以简单、高效地实现卷积层和池化层。
		
		• 通过CNN的可视化，可知随着层次变深，提取的信息愈加高级。
		
		• AlexNet是CNN的代表性网络。
		
		• 在深度学习的发展中，大数据和GPU做出了很大的贡献。
